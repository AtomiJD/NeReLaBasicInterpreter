' ==========================================================
' == An Autodiff Neural Network in jdBasic
' == Goal: Learn the XOR logic gate with high-level APIs
' ==========================================================

' --- 1. Training Data ---
' Create the raw data as standard Arrays.
TRAINING_INPUT_DATA = [[0,0], [0,1], [1,0], [1,1]]
TRAINING_OUTPUT_DATA = [[0], [1], [1], [0]]

' Convert the data Arrays into Tensors to enable gradient tracking.
TRAINING_INPUTS = TENSOR.FROM(TRAINING_INPUT_DATA)
TRAINING_OUTPUTS = TENSOR.FROM(TRAINING_OUTPUT_DATA)


' --- 2. Model and Optimizer Setup ---

' Use the new factory functions to define the network architecture and optimizer.
' This is much cleaner than manually creating weight and bias arrays.
MODEL = []
L =  TENSOR.CREATE_LAYER("DENSE", {"input_size": 2, "units": 3})
PRINT L
MODEL = APPEND(MODEL,L)

MODEL = APPEND(MODEL, TENSOR.CREATE_LAYER("DENSE", {"input_size": 3, "units": 1}))

OPTIMIZER = TENSOR.CREATE_OPTIMIZER("SGD", {"learning_rate": 0.1})
EPOCHS = 10000


' --- 3. Helper Functions ---

' The forward pass is now a generic function that can handle a model of any depth.
FUNC MODEL_FORWARD(model_array, input_tensor)
    temp_tensor = input_tensor
    FOR i = 0 TO LEN(model_array) - 1
        layer = model_array[i]
        
        ' Get the parameters from the layer's map
        weights = layer{"weights"}
        bias = layer{"bias"}
        ' Apply the layer's logic
        temp_tensor = MATMUL(temp_tensor, weights) + bias
        ' Apply activation function. Note we no longer need SIGMOID_DERIVATIVE
        temp_tensor = TENSOR.SIGMOID(temp_tensor)
    NEXT i
    RETURN temp_tensor
ENDFUNC

FUNC MSE_LOSS(predicted, actual)
    errors = actual - predicted
    squared_errors_tensor = errors ^ 2
    sum_of_squares_tensor = SUM(squared_errors_tensor)
    
    num_elements = LEN(TENSOR.TOARRAY(errors))[0]
    loss_tensor = sum_of_squares_tensor / num_elements
    RETURN loss_tensor
ENDFUNC

CLS
PRINT "--- Starting Training ---"

' --- 4. The Training Loop ---

FOR EPI = 1 TO EPOCHS
    ' --- FORWARD PASS ---
    ' Get the model's prediction.
    PREDICTED_OUTPUT = MODEL_FORWARD(MODEL, TRAINING_INPUTS)

    ' --- CALCULATE LOSS ---
    ' The result is a Tensor that is the root of our computational graph.
    LOSS_TENSOR = MSE_LOSS(PREDICTED_OUTPUT, TRAINING_OUTPUTS)
    
    ' --- BACKPROPAGATION ---
    ' This one command replaces all the manual derivative calculations!
    TENSOR.BACKWARD LOSS_TENSOR

    ' --- UPDATE WEIGHTS AND BIASES ---
    ' The optimizer handles applying all the gradients computed by BACKWARD.
    MODEL = TENSOR.UPDATE(MODEL, OPTIMIZER)

    ' Print progress every 1000 epochs
    IF EPI MOD 1000 = 0 THEN
        PRINT "Epoch:"; EPI; ", Loss:"; TENSOR.TOARRAY(LOSS_TENSOR)
    ENDIF
NEXT EPI

TENSOR.SAVEMODEL model, "lall.jmd"

PRINT
PRINT "--- Training Complete ---"
PRINT "Final Predictions:"
PRINT FRMV$(TENSOR.TOARRAY(PREDICTED_OUTPUT))

